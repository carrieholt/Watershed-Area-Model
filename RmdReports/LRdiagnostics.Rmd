---
title: "Logistic Regression Diagnostics"
author: "Carrie Holt"
date: "January 15, 2021"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE, echo=FALSE}


knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))  


library(tidyverse)
library(ggplot2)
library(viridis)
library(patchwork)

```

# Assumptions of logistic regression
1. The relationship between aggregate abundance and logit(proportions) is linear

2. The observed proportions are independent (not autocorrelated)

3. There are no influential values (outliers)

4. There is no multicollinearity among predictors (NA, only 1 predictor)

# Steps:
1. Estimate Pearson resiudals and deviance residuals (Assumption 3).
  Are deviance residuals >2?

2. (a) Plot residuals against fitted values (Assumption 1).
  Is there a trend in residuals over fitted values?
  
  (b) Plot autocorrelation among residuals Assumption 2).
  Are residuals autocorrelated?

3. Evaluate statistical significance of model coefficients.

4. Evaluate Pearson Chi-squared statistic (goodness of fit).
  Is there statistical evidence for lack of fit?

5. Evaluate Deviance G-squared statistic (goodness of fit)
  Is there statistical evidence for lack of fit?

6. Evaluate quasi-Rsquared.
  What is the ratio of the fit to the null model?

7. Evaluate Wald test.
  Is deviance significantly reduced with the predictor, Aggregate abundance?

8. Evaluate hit ratio.
  What is the classification accuracy of the LRP based on the logistic
  regression?

9. Leave-one-out cross-validaton of hit ratio.
  What is the classification accuracy of the LRP when we use out-of-sample
  predictions?
  
This code is wrapped inside a function LRdiagnostics() in a file of the same name. It's extracted here for demonstration purposes.

## The arguments of the function are:

All_Ests = Dataframe containing parameters of logistic regression, B_0 and B_1
  with p values (All_Ests <- data.frame(summary(sdreport(obj), p.value=TRUE)))

AggAbund = Vector of scaled observed aggregate abundances, with NAs removed

obsPpnAboveBM = Vectors of observed ppn of CUs above their lower benchmarks
  with NAs removed

p = the proportion of CUs that must be > their benchmark when defining LRP

nLL = negLogLikehood = ans from TMB file, outputted with REPORT(ans); in TMB
  and then called in R with obj$report()$ans

dir = name of directory where plots should be saved followed by /

plotname = filename for residual plots

First input data are loaded. 

Step 1. What are residuals, and are deviance residuals > 2?

```{r warning=FALSE, message=FALSE, error=FALSE}
load("DataIn/Input_LRdiagnostics.rda")
All_Ests <- input$All_Ests
AggAbund <- input$AggAbund
obsPpnAboveBM <- input$obsPpnAboveBM
p <- input$p
nLL <- input$nLL
dir <- input$dir
plotname <- input$plotname

source("R/helperFunctions.r") # contains function ggplot.corr()



 #-------------------------------------------------------------------------------
  # Step 1. Estimate Pearson resiudals and deviance residuals (Assumption 3). 
  #   Are deviance residuals >2?
  #-------------------------------------------------------------------------------
  
  # Get observed and predicted ppn of CUs above their lower benchmark
  B_0 <- All_Ests %>% filter(Param=="B_0") %>% pull(Estimate)
  B_1 <- All_Ests %>% filter(Param=="B_1") %>% pull(Estimate)
  predPpnAboveBM <- inv_logit(B_0 + B_1*AggAbund)
  
  
  # Pearson residuals: Eq3.15 https://data.princeton.edu/wws509/notes/c3s8
  # setting n=1 (number of trials at each observation of x)
  
  PearResid <- ( obsPpnAboveBM - predPpnAboveBM ) / sqrt( predPpnAboveBM * 
                                                            (1 - predPpnAboveBM) ) 

  PearResid
  # Deviance residual: Eq3.16 https://data.princeton.edu/wws509/notes/c3s8
  # setting n=1 (number of trials at each observation of x)
  # To avoid NANs, use ifelse statement in the eqn, as suggested here:
  # https://www.datascienceblog.net/post/machine-learning/interpreting_
  # generalized_linear_models/#:~:text=For%20type%20%3D%20%22pearson%22%2
  # 0%2C,%CB%86f(xi)
  
  binom.resid <- function(y, mu) {
    y * log( ifelse(y== 0, 1, y/mu)) + (1-y) * log( 
      ifelse(y==1 ,1,(1-y)/(1-mu) ) )  
  }
  
  
  DevResid <- sign(obsPpnAboveBM - predPpnAboveBM ) * 
    sqrt( 2 * binom.resid(y=obsPpnAboveBM, mu=predPpnAboveBM) ) 
  

  DevResid
  # Observations with a deviance residual in excess of two may indicate 
  # lack of fit. (https://data.princeton.edu/wws509/notes/c3s8)
  
  ## Testing. Residuals match output from R using glm objects
  # ModDat <- data.frame(xx=data$LM_Agg_Abund, yy=SMUlogisticData$ppn)
  # Fit_Mod <- glm( yy ~ xx , family = quasibinomial, data=ModDat)
  # B_0 <- Fit_Mod$coef[1]
  # B_1 <- Fit_Mod$coef[2]
  # obsPpnAboveBM <- ModDat$yy
  # predPpnAboveBM <- inv_logit(B_0 + B_1*ModDat$xx)
  # residuals(Fit_Mod, type="pearson")
  # residuals(Fit_Mod, type="deviance")


```

2. Then residuals are plotted. Is there a trends over fitted values?
```{r}
  #-------------------------------------------------------------------------------
  # Step 2:
  # 2a. Plot residuals against fitted values (Assumption 1). 
  #   Is there a trend in residuals over fitted values?
  # 2b. Plot autocorrelation among residuals Assumption 2). 
  #   Are residuals autocorrelated?
  #-------------------------------------------------------------------------------
  
  
  # Put data for diagnostics in a dataframe for plotting
  diagData <- data.frame(predPppnAboveBM = predPpnAboveBM, 
                         PearResid = PearResid, DevResid=DevResid)
  
  p1 <- ggplot(diagData, aes(predPpnAboveBM, PearResid)) +
    geom_point(size=3) + 
    #geom_smooth(method="lm", formula=y~x) + 
    geom_smooth(method="loess", formula=y~x, span=1) + 
    geom_hline(linetype="dashed", yintercept=0) +
    xlab("Predicted proportions") + ylab("Pearson's Residuals") +
    ggtitle("Pearson's Residuals") +
    theme_classic() + 
    theme(axis.text=element_text(size=12),
          axis.title=element_text(size=14,face="bold"),
          plot.title = element_text(size = 20)
    ) 
  
  p2 <- ggplot(diagData, aes(predPpnAboveBM, DevResid)) +
    geom_point(size=3) + 
    #geom_smooth(method="lm", formula=y~x) + 
    geom_smooth(method="loess", formula=y~x, span=1) + 
    geom_hline(linetype="dashed", yintercept=0) +
    xlab("Predicted proportions") + ylab("Deviance Residuals") +
    ggtitle("Deviance Residuals") +
    theme_classic() + 
    theme(axis.text=element_text(size=12),
          axis.title=element_text(size=14,face="bold"),
          plot.title = element_text(size = 20)
    ) 
  
  # See ggplot.cor function in "helperFunctions.r"
  p3 <- ggplot.corr(data=PearResid, title="Pearsons's residuals") 
  p4 <- ggplot.corr(data=DevResid, title="Deviance residuals") 

  p1+p2+p3+p4
```

3. Are coefficients significant?
```{r}
  #-------------------------------------------------------------------------------
  # step 3. 
  # Evaluate statistical significance of model coefficients.
  #-------------------------------------------------------------------------------
  
  signTable <-  All_Ests %>% filter(Param %in% c("B_0", "B_1")) %>% 
    rename(P.value=Pr...z.2..) %>% dplyr::select(Param, Estimate, 
                                                 Std..Error, z.value, 
                                                 P.value)
  
  signTable

```

4. What is goodness of fit based on Pearson chi-square?
```{r}
 #-------------------------------------------------------------------------------
  # Step 4. 
  # Evaluate Pearson Chi-square statistic (goodness of fit). 
  #   Is there statistical evidence for lack of fit?
  #-------------------------------------------------------------------------------
  
  # Evaluate goodness of fit by comparing the residual deviance to a Chi-square
  # distribution  
  
  # Sum of squared Pearson's residuals
  Pearson <- sum(PearResid^2)
  
  # Statistical test of the goodness of fit 
  # Section 6.5.6. https://bookdown.org/roback/bookdown-bysh/ch-logreg.html
  # Section 1.4 of https://www.flutterbys.com.au/stats/tut/tut10.5a.html
  p.PearChiSq <- 1 - pchisq(q=Pearson, df=length(PearResid)-2)
  #values < 0.05 indicate statistically significant evidence for lack of fit
  names(p.PearChiSq) <- c("PearChiSq")
  p.PearChiSq

```

5. What is goodness of fit based on Deviance G-squared? This statistics measures the
deviance of the fitted logistic model with respect to a perfect model (the saturated model)
```{r}
 #-------------------------------------------------------------------------------
  # Step 5. 
  # Evaluate Deviance G-squared statistic (goodness of fit)
  #   Is there statistical evidence for lack of fit?
  #-------------------------------------------------------------------------------
  
  # Deviance statistic (sum of deviance residuals)
  # The deviance is a key concept in logistic regression. It measures the
  # deviance of the fitted logistic model with respect to a perfect model (the 
  # saturated model) https://bookdown.org/egarpor/SSS2-UC3M/logreg-deviance.html
  Deviance <- sum(DevResid^2)
  p.DevChiSq <- 1-pchisq(q=Deviance, df=length(DevResid)-2)
  # values < 0.05 indicate statistically significant evidence for lack of fit
  # See section 6.5.6: https://bookdown.org/roback/bookdown-bysh/ch-logreg.html
  names(p.DevChiSq) <- c("p.DevChiSq")
  
  p.DevChiSq
```

6. What is quasiR2? This is not the percentage of variance explained by the logistic model, but rather a ratio indicating how close is the fit to being perfect  or the worst.
```{r}
  #-------------------------------------------------------------------------------
  # Step 6. 
  # Evaluate quasi-Rsquared.
  #   What is the ratio of the fit to the null model?
  #-------------------------------------------------------------------------------
  
  # What is ratio of fit to the null model? This is a measure of the
  # strength of the relationship
  NullDev <- deviance(glm( obsPpnAboveBM ~ 1 , family = 
                             quasibinomial))
  quasiR2 <- 1 - Deviance/NullDev
  # This is not the percentage of variance explained by the logistic model, 
  # but rather a ratio indicating how close is the fit to being perfect 
  # or the worst. It is not related to any correlation coefficient
  # https://bookdown.org/egarpor/SSS2-UC3M/logreg-deviance.html
  # However, when we forced the y-intercept near zero,  the model actually fits 
  # worse than the null model and quasiR2 can be negative
  names(quasiR2) <- c("quasiR2")
  
  quasiR2

  
  
```

7. Wald test

```{r}
  #-------------------------------------------------------------------------------
  # Step 7. 
  # Evaluate Wald test
  #   Is the predictor, 'Aggregate Abundance', a significant predictor of the ppn
  #   of CUs > their lower benchmark?
  #-------------------------------------------------------------------------------
  
  # The Wald test evaluates the significance of a predictor based on difference 
  # in Deviances between the specificed model and a reduced model without the 
  # predictor
  # See Section 4.7 https://bookdown.org/egarpor/SSS2-UC3M/logreg-deviance.html
  p.Wald <- signif( pchisq(q=(Deviance-NullDev), df=1), digits=2)
  names(p.Wald) <- c("p.Wald")
  p.Wald
```

8. What is the classifiation accuracy?
```{r}
  #-----------------------------------------------------------------------------
  # Step 8. 
  # Evaluate hit ratio from a confusion matrix
  #   What is the classification accuracy of the LRP based on the logistic 
  #   regression?
  #-----------------------------------------------------------------------------
  
  
  # In which years did the model predict aggregate abundances >LRP?
  yHat <- predPpnAboveBM > p
  # In which years were observed aggregate abundances >LRP?
  y <- obsPpnAboveBM > p
  
  # Confusion Matrix
  confMat <- table(y, yHat)
  confMat
  
  # What is the accuracy in classifying observed aggregate abundances?
  # Hit ratio = ratio of correct classification
  hitRatio <- sum(diag(confMat))/sum(confMat)
  hitRatio <- round(hitRatio, digits=2)
  names(hitRatio) <- c("hitRatio")
  hitRatio
  
```

9. Leave-one-out cross validation:  classification accuracy
Sequentially remove one data point at a time, re-estimate logistic regression and LRP, and then calculate Hit Ratio (classification accurarcy) by comparing predicted and observed values for the data point that was held out. See LRdiagnostics.R for details on how to adapt this diagnostic for other case studies.

```{r}
  #------------------------------------------------------------------------------
  # Step 9.
  # Evaluate hit ratio using leave-one-out cross validation from a confusion
  # matrix. Contains 4 sub-steps, 9.1-9.4.
  #   What is the classification accuracy of the LRP using out-of-sample data?
  #------------------------------------------------------------------------------


  # Step 9.1: Estimate logistic regression iteratively, removing a single year
  # each time

  n <- 18 # length of time-series used in logistic regression
  source("R/WCVILRPs.R") # required to use the Get.LRP() function for the 
  # WCVI case study
  
  predPpnAboveBM <- NA

  for (i in 1:n){
    # Estimate logistic regression using function Get.LRP
    zz <- Get.LRP(remove.EnhStocks = TRUE, LOO=i)
    All_Ests <- zz$out$All_Ests

    if(i==1){ # These remain constant over iterations
      # Step 9.2: Get observed time-series of aggregate raw abundances that includes all
      # data and then scale to units near 1-10
      AggAbundRaw <- zz$out$Logistic_Data$xx
      digits <- count.dig(AggAbundRaw)
      ScaleSMU <- min(10^(digits -1 ), na.rm=T)
      AggAbund <- AggAbundRaw/ScaleSMU
      # Get time-series of observed ppns of CUs> benchamark, including all
      # data
      obsPpnAboveBM <- zz$out$Logistic_Data$yy
      # Get threshold p value (ppn of CUs>benchmark) used to estimate LRP
      p <- zz$LRPppn
      #dir <- "DataOut/"
    }

   # Step 9.3: Get predicted ppn of CUs above their lower benchmark for the year
    # that was held out
    B_0 <- All_Ests %>% filter(Param=="B_0") %>% pull(Estimate)
    B_1 <- All_Ests %>% filter(Param=="B_1") %>% pull(Estimate)
    #predPpnAboveBM <- inv_logit(B_0 + B_1*AggAbund)
    predPpnAboveBM[i] <- inv_logit(B_0 + B_1*AggAbund[i])
  } # End of for i in 1:18

  # Step 9.4: Calculate Hit Ratio

  # In which years did the model predict aggregate abundances >LRP?
  yHat <- predPpnAboveBM > p
  # In which years were observed aggregate abundances >LRP?
  y <- obsPpnAboveBM > p

  # Confusion Matrix
  confMat <- table(y, yHat)

  # What is the accuracy in classifying observed aggregate abundances?
  # Hit ratio = ratio of correct classification
  hitRatio <- sum(diag(confMat))/sum(confMat)
  hitRatio <- round(hitRatio, digits=2)

  hitRatio

```

