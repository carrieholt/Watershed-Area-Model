---
title: "Logistic Regression Diagnostics"
author: "Carrie Holt"
date: "April 1, 2021"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE, echo=FALSE}

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))  



library(tidyverse)
library(ggplot2)
library(viridis)
library(patchwork)
library(TMB)

```

# Assumptions of logistic regression
1. The relationship between aggregate abundance and logit(proportions) or log-odds is linear.

2. The observations are independent of each other (i.e., residuals are not autocorrelated).

3. There are no influential outliers. 

4. The sample size is sufficiently large. Logistic regression assumes that the sample size of the dataset if large enough to draw valid conclusions from the fitted logistic regression model.

5. There is no multicollinearity among predictors (NA, only 1 predictor)

# Steps:
1. Box-Tidwell test to assess linearity (Assumption 1). 

2. Estimate Pearson residuals and deviance residuals, and plot residuals against fitted values. 

3. Plot autocorrelation among residuals. Are residuals autocorrelated? (Assumption 2)

4. Are deviance residuals >3, i.e., are they outliers? (Assumption 3)  

5. Evaluate if sample size is sufficient. 

6. Evaluate statistical significance of model coefficients using Wald's Test.

7. Evaluate goodness-of-fit based on ratio of Deviance to the null model (~likelihood-ratio test).

8. Evaluate quasi-Rsquared.
  What is the ratio of the fit to the null model?

9. Evaluate hit ratio.
  What is the classification accuracy of the LRP based on the logistic
  regression?

10. Leave-one-out cross-validation of hit ratio.
  What is the classification accuracy of the LRP when we use out-of-sample
  predictions?
  
This code to run the steps is wrapped inside a function LRdiagnostics() in a file of the same 
name. It's extracted here for demonstration purposes.

## The arguments of the function are:

SMUlogisticData = Dataframe containing columns: Years, SMU_Esc (annual aggregate escapement to the SMU), and ppn (vector of 0 and 1's indicating if all CUs are above the lower benchmark (1) or not (0); also accepts  proportion of CUs that are above their lower benchmark in each year).

CU_Names = the names of the CUs

All_Ests = Dataframe containing parameters of logistic regression, B_0 and B_1
  with p values (All_Ests <- data.frame(summary(sdreport(obj), p.value=TRUE)))

AggAbund = Vector of scaled observed aggregate abundances, with NAs removed

obsPpnAboveBM = Vectors of 0 and 1's representing if all CUs above their lower benchmarks (1) or not (0) for         Bernoulli regression with NAs removed, or the proportion of CUs above their lower benchmark for binomial           regression

p = the proportion of CUs that must be > their benchmark when defining LRP, for binomial regression only

nLL = negLogLikehood = ans from TMB file, outputted with REPORT(ans); in TMB
  and then called in R with obj$report()$ans

dir = name of directory where plots should be saved followed by /

plotname = filename for residual plots



Step 1.  Box-Tidwell test to assess linearity (Assumption 1). First, data to run the logistic regression are loaded The rda file contain a list, "SMUlogisticData", and vector "CU_Names", and is saved on a "DataIn" directory.

```{r}
load("DataIn/BoxTidwellInput.rda")
#load("BoxTidwellInput.rda")
SMUlogisticData
CU_Names

```

Then generate inputs for logistic regression and run the regression with an additional interaction term, aggregate abundances x ln(aggregate abundance). A significant interaction term indicates that the relationship is not linear.
This code requires that a "Logistic_LRPs_BoxTidwell.cpp" file with a logistic regression that includes this interaction term has been compiled and is in the directory "TMB_Inputs". A significant Box-Tidwell statistic indicates a lack of linearity in the relationship between aggregate abundances and log-odds.

For references, see pdf at bottom of https://www.researchgate.net/post/Thesis-Help-Binary-Logistic-Regression-Linearity-Assumption
or http://www.statisticalassociates.com/logistic10.htm

<!--I have requested Fox et al. 2015 from the library. Another option for a reference (could request this from library) Fox et al. 2019 https://us.sagepub.com/en-us/nam/an-r-companion-to-applied-regression/book246125#contents-->


```{r boxTidwell, warning=FALSE, message=FALSE, error=FALSE}
  #-------------------------------------------------------------------------------
  # Step 1. Box-Tidwell test to assess linearity between aggregate abundances
  #       and log-odds of all CUs being above thier lower benchmarks. 
  #-------------------------------------------------------------------------------


  source("R/helperFunctions.r")

  Bern_logistic <- FALSE # For this dummy example on WCVI CK, otherwise set to TRUE

  data <- list()
  data$N_Stks <- length(CU_Names)
  digits <- count.dig(SMUlogisticData$SMU_Esc)
  ScaleSMU <- min(10^(digits -1 ), na.rm=T)
  
  data$LM_Agg_Abund <- SMUlogisticData$SMU_Esc/ScaleSMU
  data$LM_Agg_AbundxLn <- SMUlogisticData$SMU_Esc/ScaleSMU * 
    log(SMUlogisticData$SMU_Esc/ScaleSMU)
  data$N_Above_BM <- SMUlogisticData$ppn * data$N_Stks
  data$Pred_Abund <- seq(0, max(data$LM_Agg_Abund)*1.5, 0.1)
  data$p <- 0.95#0.67
  data$Penalty <- as.numeric(FALSE)# Penalty is not relevant with B_2 term.

  data$Bern_logistic <- as.numeric(Bern_logistic)

  data$B_penalty_mu <- mean(c(200,9962))/ScaleSMU
  data$B_penalty_sig <- 2400/ScaleSMU
    
  param <- list()
  param$B_0 <- -2
  param$B_1 <- 0.1
  param$B_2 <- 0.1
  
  dyn.load(dynlib(paste("TMB_Files/Logistic_LRPs_BoxTidwell", sep="")))
    
  obj <- MakeADFun(data, param, DLL="Logistic_LRPs_BoxTidwell", silent=TRUE)
    
  opt <- nlminb(obj$par, obj$fn, obj$gr, 
                control = list(eval.max = 1e5, iter.max = 1e5))
  pl <- obj$env$parList(opt$par) 
  #summary(sdreport(obj), p.value=TRUE)
  All_Ests <- data.frame(summary(sdreport(obj), p.value=TRUE))

  All_Ests$Param <- row.names(All_Ests)
  All_Ests$Param <- sapply(All_Ests$Param, function(x) 
    (unlist(strsplit(x, "[.]"))[[1]]))
    
  BoxTidwellp <- All_Ests %>% filter(Param=="B_2") %>% pull(Pr...z.2..)
  BoxTidwellp <- round(BoxTidwellp,2)    
  names(BoxTidwellp) <- c("Box-Tidwell p-value")
  BoxTidwellp

```

2. Estimate Pearson residuals and deviance residuals, and plot residuals against fitted values. First data are inputted from "Input_LRdiagnostics.rda".


```{r ResidPlot, fig.cap="Plot of Pearson and Deviance residuals against fitted values. Blue line is loess smoothed residuals and grey band is 95 percent confidence intervals.",warning=FALSE, message=FALSE, error=FALSE}

load("DataIn/Input_LRdiagnostics.rda")
All_Ests <- input$All_Ests
AggAbund <- input$AggAbund
obsPpnAboveBM <- input$obsPpnAboveBM
p <- input$p
nLL <- input$nLL
dir <- input$dir
plotname <- input$plotname

source("R/helperFunctions.r") # contains function ggplot.corr()



 #-------------------------------------------------------------------------------
  # Step 2. Estimate Pearson resiudals and deviance residuals. 
  #-------------------------------------------------------------------------------
  
  # Get observed and predicted ppn of CUs above their lower benchmark
  B_0 <- All_Ests %>% filter(Param=="B_0") %>% pull(Estimate)
  B_1 <- All_Ests %>% filter(Param=="B_1") %>% pull(Estimate)
  predPpnAboveBM <- inv_logit(B_0 + B_1*AggAbund)
  
  
  # Pearson residuals: Eq3.15 https://data.princeton.edu/wws509/notes/c3s8
  # setting n=1 (number of trials at each observation of x)
  
  PearResid <- (obsPpnAboveBM - predPpnAboveBM) / sqrt(predPpnAboveBM * 
                                                          (1 - predPpnAboveBM)) 

  # Deviance residual: Eq3.16 https://data.princeton.edu/wws509/notes/c3s8
  # setting n=1 (number of trials at each observation of x)
  # To avoid NANs, use ifelse statement in the eqn, as suggested here:
  # https://www.datascienceblog.net/post/machine-learning/interpreting_
  # generalized_linear_models/#:~:text=For%20type%20%3D%20%22pearson%22%2
  # 0%2C,%CB%86f(xi)
  
  binom.resid <- function(y, mu) {
    y * log( ifelse(y== 0, 1, y/mu)) + (1-y) * log( 
      ifelse(y==1 ,1,(1-y)/(1-mu) ) )  
  }
  
  
  DevResid <- sign(obsPpnAboveBM - predPpnAboveBM ) * 
    sqrt( 2 * binom.resid(y=obsPpnAboveBM, mu=predPpnAboveBM) ) 
  

   
  ## Testing. Residuals match output from R using glm objects
  # ModDat <- data.frame(xx=data$LM_Agg_Abund, yy=SMUlogisticData$ppn)
  # Fit_Mod <- glm( yy ~ xx , family = quasibinomial, data=ModDat)
  # B_0 <- Fit_Mod$coef[1]
  # B_1 <- Fit_Mod$coef[2]
  # obsPpnAboveBM <- ModDat$yy
  # predPpnAboveBM <- inv_logit(B_0 + B_1*ModDat$xx)
  # residuals(Fit_Mod, type="pearson")
  # residuals(Fit_Mod, type="deviance")

  # Put data for diagnostics in a dataframe for plotting
  diagData <- data.frame(predPppnAboveBM = predPpnAboveBM, 
                         PearResid = PearResid, DevResid=DevResid)
  
  p1 <- ggplot(diagData, aes(predPpnAboveBM, PearResid)) +
    geom_point(size=3) + 
    #geom_smooth(method="lm", formula=y~x) + 
    geom_smooth(method="loess", formula=y~x, span=1) + 
    geom_hline(linetype="dashed", yintercept=0) +
    xlab("Predicted proportions") + ylab("Pearson's Residuals") +
    ggtitle("Pearson's Residuals") +
    theme_classic() + 
    theme(axis.text=element_text(size=12),
          axis.title=element_text(size=14,face="bold"),
          plot.title = element_text(size = 20)
    ) 
  
  p2 <- ggplot(diagData, aes(predPpnAboveBM, DevResid)) +
    geom_point(size=3) + 
    #geom_smooth(method="lm", formula=y~x) + 
    geom_smooth(method="loess", formula=y~x, span=1) + 
    geom_hline(linetype="dashed", yintercept=0) +
    xlab("Predicted proportions") + ylab("Deviance Residuals") +
    ggtitle("Deviance Residuals") +
    theme_classic() + 
    theme(axis.text=element_text(size=12),
          axis.title=element_text(size=14,face="bold"),
          plot.title = element_text(size = 20)
    ) 
p1+p2

```

3. Plot autocorrelation among residuals. Are residuals autocorrelated? (Assumption 2)
```{r autocorrPlot, fig.cap="Plot of autocorrelation coefficients for Pearson and Deviance residuals.",warning=FALSE, message=FALSE, error=FALSE}
  #-------------------------------------------------------------------------------
  # Step 3:
  #  Plot autocorrelation among residuals. Are residuals autocorrelated? 
  #   (Assumption 2)
  #-------------------------------------------------------------------------------

 
  # See ggplot.cor function in "helperFunctions.r"
  p3 <- ggplot.corr(data=PearResid, title="Pearsons's residuals") 
  p4 <- ggplot.corr(data=DevResid, title="Deviance residuals") 

  p3+p4

```

4. Are any residuals outliers? (i.e., are deviance residuals >3) (Assumption 3).  Note, we cannot yet evaluate influence test statistics such as Cook's distance because the hat  matrix is not provided in TMB outputs. 


```{r warning=FALSE, message=FALSE, error=FALSE}
  #-------------------------------------------------------------------------------
  # Step 4:
  #  Are any residuals outliers? (i.e., are deviance residuals >3, Assumption 3)
  #   Note, we cannot evaluate influence test statistics such as Cook's 
  #   distance because the hat matrix is not provided in TMB outputs
  #-------------------------------------------------------------------------------
  
  # Observations with a deviance residual in excess of 3 indicate outlier, 
  # Ahmad et al. 2011 (https://www.researchgate.net/publication/260368584_Diagnostic_for_Residual_Outliers_using_Deviance_Component_in_Binary_Logistic_Regression)

  # Possible stricter criteria: Observations with a deviance residual in excess 
  # of two may indicate lack of fit. 
  # https://data.princeton.edu/wws509/notes/c3s8

  # See figure of residuals vs fitted values
  # See figures here, which show that deviance residuals for logistic regression can be >1
  # https://stats.idre.ucla.edu/stata/webbooks/logistic/chapter3/lesson-3-logistic-regression-diagnostics-2/

  # maximum absolute residuals values < 3
  round(max(abs(PearResid)),2)

  round(max(abs(DevResid)), 2)


```
5. Evaluate if sample size is sufficient. As a rule of thumb, Peduzzi et al.(1996) suggests a minimum of 10 cases with the least frequent outcome for each explanatory variable (1 in this case). For example, if the expected probabilities are 0.50 and 0.50 (for 0 and 1, respectively), then the minimum sample size of at least (10*1) / 0.50 = 20 to avoid biases in model coefficients. 

```{r warning=FALSE, message=FALSE, error=FALSE}
  #-------------------------------------------------------------------------------
  # Step 5:
    #  Evaluate if sample size is sufficient
  #-------------------------------------------------------------------------------

  # As a rule of thumb, as a rule of thumb, Peduzzi et al.(1996) suggests a 
  # minimum of 10 cases with the least frequent outcome for each explanatory 
  # variable (1 in this case). For example, if the expected probabilities are 
  # 0.50 and 0.50 (for 0 and 1, respectively), then the minimum sample size of 
  #a t least (10*1) / 0.50 = 20 to avoid biases in model coefficients. 
  # https://www.statology.org/assumptions-of-logistic-regression/

# Frequency of outcomes (coded to accept proportional as well as 0/1 data)

Freq <- c(sum(floor(SMUlogisticData$ppn))/length(SMUlogisticData$ppn),
          sum(ceiling(SMUlogisticData$ppn))/length(SMUlogisticData$ppn))

minFreq <- min(Freq)
minSampleSize <- 10/min(Freq)

sampleSize <- length(SMUlogisticData$ppn)

if (minSampleSize > sampleSize) print("Sample is below minimum suggested levels") else print("Sample size is sufficient")


```

6. Evaluate statistical significance of model coefficients using Wald's Test.
```{r}
  #-------------------------------------------------------------------------------
  # step 6. 
  # Evaluate statistical significance of model coefficients using Wald's test,
  # where Wald's statistic = coefficient/SE(coefficient))
  # Statistical significant is shown in p-value from model output.
  #-------------------------------------------------------------------------------
  
  signTable <-  All_Ests %>% filter(Param %in% c("B_0", "B_1")) %>% 
    rename(P.value=Pr...z.2..) %>% dplyr::select(Param, Estimate, 
                                                 Std..Error, z.value, 
                                                 P.value)
  
  signTable

```


7. Evaluate the overall goodness-of-fit of the logistic model based ratio of Deviance to the null model (~likelihood-ratio test)
```{r}
 #-------------------------------------------------------------------------------
  # Step 7. 
  # Evaluate goodness-of-fit based on ratio of Deviance to the null model 
  #   Is there statistical evidence for lack of fit?
 #-------------------------------------------------------------------------------
  
  # Evaluate goodness of fit by comparing the residual deviance to null 
  # deviance, and evaluating this ratio relative to  a Chi-square distribution 
  # (df = 1, the difference in the number of parameters) 
  # P < 0.05 indicates a significant lack of fit.
  # Agresti et al. 2007 (LRT); Ahmad et al. 2011 (definition of deviance)
  

  Deviance <- sum(DevResid^2)

  NullDev <- deviance(glm( obsPpnAboveBM ~ 1 , family = 
                             quasibinomial))

  p.DRT <- signif( pchisq(q= - (NullDev-Deviance), df=1), digits=2)
  
  names(p.DRT) <- c("p.DRT")
  # There is no evidence for lack of fit.
  
  
  # Note, Roback and Legler 2021 suggest evaluating overall model fit based on 
  # chi-square distribution of the deviance itself, df=n-p
  
  # values < 0.05 indicate statistically significant evidence for lack of fit
  # See section 6.5.6: https://bookdown.org/roback/bookdown-bysh/ch-logreg.html
  
  
  # p.DevChiSq <- 1-pchisq(q=Deviance, df=length(DevResid)-2)
  # names(p.DevChiSq) <- c("p.DevChiSq")
  # p.DevChiSq



  # Or equivalently, based on Pearson residuals, where sum of squared 
  # Pearson's residuals is,
  # Pearson <- sum(PearResid^2)
  # p.PearChiSq <- 1 - pchisq(q=Pearson, df=length(PearResid)-2)
  # names(p.PearChiSq) <- c("PearChiSq")
  # p.PearChiSq
  # Section 1.4 of https://www.flutterbys.com.au/stats/tut/tut10.5a.html
  
  # Note, the deviance is defined as the difference of likelihoods 
  # between the fitted model and the saturated model:
  # D = − 2 loglik(^β) + 2 loglik(saturated model), 
  # where loglik(saturated model) = 1 and so is dropped from the equation
  # Portugués et al. 2020 (online resource only)
  # https://bookdown.org/egarpor/SSS2-UC3M/logreg-deviance.html (section 4.7)


```

8. What is quasiR2? This is a ratio indicating how close is the fit to being perfect or the worst (not the percentage of variance explained by the logistic model, as in linear models)
```{r}
  #-------------------------------------------------------------------------------
  # Step 6. 
  # Evaluate quasi-Rsquared.
  #   What is the ratio of the fit to the null model?
  #-------------------------------------------------------------------------------
  
  # What is ratio of fit to the null model? This is a measure of the
  # strength of the relationship
  NullDev <- deviance(glm( obsPpnAboveBM ~ 1 , family = 
                             quasibinomial))
  quasiR2 <- 1 - Deviance/NullDev
  # This is not the percentage of variance explained by the logistic model, 
  # but rather a ratio indicating how close is the fit to being perfect 
  # or the worst. It is not related to any correlation coefficient
  # https://bookdown.org/egarpor/SSS2-UC3M/logreg-deviance.html
  # However, when we forced the y-intercept near zero,  the model actually fits 
  # worse than the null model and quasiR2 can be negative
  names(quasiR2) <- c("quasiR2")
  
  quasiR2

  
  
```

??. Wald test
Is this likelihood ration test? Do I need the likelihoods instead of Deviance? If so, how doI get likelihood? Is 'Pearson' sufficient?

```{r}
  #-------------------------------------------------------------------------------
  # Step ??. 
  # Evaluate Wald test
  #   Is the predictor, 'Aggregate Abundance', a significant predictor of the ppn
  #   of CUs > their lower benchmark?
  #-------------------------------------------------------------------------------
  
  # The Wald test evaluates the significance of a predictor based on difference 
  # in Deviances between the specificed model and a reduced model without the 
  # predictor
  # See Section 4.7 https://bookdown.org/egarpor/SSS2-UC3M/logreg-deviance.html
  p.Wald <- signif( pchisq(q=(Deviance-NullDev), df=1), digits=2)
  names(p.Wald) <- c("p.Wald")
  p.Wald
  
  
  # CHECK THESE BEFORE DELETING THIS SECTION
  
  #https://bookdown.org/egarpor/SSS2-UC3M/logreg-deviance.html
  
  
  # https://www.sciencedirect.com/topics/mathematics/wald-test#:~:text=The%20test%20statistic%20for%20the,follows%20a%20standard%20normal%20distribution.
  
  # https://bookdown.org/egarpor/SSS2-UC3M/logreg-inference.html
  # https://bookdown.org/egarpor/SSS2-UC3M/multlin-inference.html#eq:normp2
  
  # http://web.pdx.edu/~newsomj/mlrclass/ho_significance.pdf
  
  # https://bookdown.org/roback/bookdown-BeyondMLR/ch-logreg.html
  
  # check Wald agaisnt LR test: https://www.ics.uci.edu/~dgillen/STAT211/Handouts/lecture10.pdf
  
  # overall approach: https://online.stat.psu.edu/stat501/lesson/15/15.4
  
```

9. What is the classifiation accuracy?
```{r}
  #-----------------------------------------------------------------------------
  # Step 9. 
  # Evaluate hit ratio from a confusion matrix
  #   What is the classification accuracy of the LRP based on the logistic 
  #   regression?
  #-----------------------------------------------------------------------------
  
  
  # In which years did the model predict aggregate abundances >LRP?
  yHat <- predPpnAboveBM > p
  # In which years were observed aggregate abundances >LRP?
  y <- obsPpnAboveBM > p
  
  # Confusion Matrix
  confMat <- table(y, yHat)
  confMat
  
  # What is the accuracy in classifying observed aggregate abundances?
  # Hit ratio = ratio of correct classification
  hitRatio <- sum(diag(confMat))/sum(confMat)
  hitRatio <- round(hitRatio, digits=2)
  names(hitRatio) <- c("hitRatio")
  hitRatio
  
```

10. Leave-one-out cross validation:  classification accuracy
Sequentially remove one data point at a time, re-estimate logistic regression and LRP, and then calculate Hit Ratio (classification accurarcy) by comparing predicted and observed values for the data point that was held out. See LRdiagnostics.R for details on how to adapt this diagnostic for other case studies.

```{r}
  #------------------------------------------------------------------------------
  # Step 10.
  # Evaluate hit ratio using leave-one-out cross validation from a confusion
  # matrix. Contains 4 sub-steps, 9.1-9.4.
  #   What is the classification accuracy of the LRP using out-of-sample data?
  #------------------------------------------------------------------------------


  # Step 9.1: Estimate logistic regression iteratively, removing a single year
  # each time

  n <- 18 # length of time-series used in logistic regression
  source("R/WCVILRPs.R") # required to use the Get.LRP() function for the 
  # WCVI case study
  
  predPpnAboveBM <- NA

  for (i in 1:n){
    # Estimate logistic regression using function Get.LRP
    zz <- Get.LRP(remove.EnhStocks = TRUE, LOO=i)
    All_Ests <- zz$out$All_Ests

    if(i==1){ # These remain constant over iterations
      # Step 9.2: Get observed time-series of aggregate raw abundances that includes all
      # data and then scale to units near 1-10
      AggAbundRaw <- zz$out$Logistic_Data$xx
      digits <- count.dig(AggAbundRaw)
      ScaleSMU <- min(10^(digits -1 ), na.rm=T)
      AggAbund <- AggAbundRaw/ScaleSMU
      # Get time-series of observed ppns of CUs> benchamark, including all
      # data
      obsPpnAboveBM <- zz$out$Logistic_Data$yy
      # Get threshold p value (ppn of CUs>benchmark) used to estimate LRP
      p <- zz$LRPppn
      #dir <- "DataOut/"
    }

   # Step 9.3: Get predicted ppn of CUs above their lower benchmark for the year
    # that was held out
    B_0 <- All_Ests %>% filter(Param=="B_0") %>% pull(Estimate)
    B_1 <- All_Ests %>% filter(Param=="B_1") %>% pull(Estimate)
    #predPpnAboveBM <- inv_logit(B_0 + B_1*AggAbund)
    predPpnAboveBM[i] <- inv_logit(B_0 + B_1*AggAbund[i])
  } # End of for i in 1:18

  # Step 9.4: Calculate Hit Ratio

  # In which years did the model predict aggregate abundances >LRP?
  yHat <- predPpnAboveBM > p
  # In which years were observed aggregate abundances >LRP?
  y <- obsPpnAboveBM > p

  # Confusion Matrix
  confMat <- table(y, yHat)

  # What is the accuracy in classifying observed aggregate abundances?
  # Hit ratio = ratio of correct classification
  hitRatio <- sum(diag(confMat))/sum(confMat)
  hitRatio <- round(hitRatio, digits=2)

  hitRatio

```

