---
title: "Logistic Regression Diagnostics"
author: "Carrie Holt"
date: "April 1, 2021"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE, echo=FALSE}


knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath(".."))  


library(tidyverse)
library(ggplot2)
library(viridis)
library(patchwork)

```

# Assumptions of logistic regression
1. The relationship between aggregate abundance and logit(proportions) or log-odds is linear.

2. The observations are independent of each other (i.e., residuals are notautocorrelated).

3. There are no influential outliers. 

4. The sample size is sufficiently large. Logistic regression assumes that the sample size of the dataset if large enough to draw valid conclusions from the fitted logistic regression model.

5. There is no multicollinearity among predictors (NA, only 1 predictor)

# Steps:
1. Box-Tidwell test to assess linearity (Assumption 1). 

2. Estimate Pearson residuals and deviance residuals, and plot residuals against fitted values. 

3. Plot autocorrelation among residuals. Are residuals autocorrelated? (Assumption 2)

4. Are deviance residuals >3, i.e., are they outliers? (Assumption 3)  

5. Evaluate if sample size is sufficient. As a rule of thumb, Agresti (2007) suggests a minimum of 10 cases with the least frequent outcome for each explanatory variable (1 in this case). For example, if the expected probabilities are 0.50 and 0.50 (for 0 and 1, respectively), then the minimum sample size of at least (10*1) / 0.50 = 20. (https://www.statology.org/assumptions-of-logistic-regression/)

6. Evaluate statistical significance of model coefficients using Wald's Test.

7. Evaluate Likelihood-Ratio (or Deviance) Test. (~similar to statistical significance of model coefficient for models with 1 independent variable)

8. Evaluate goodness-of-fit overall, using (a) Pearson Chi-squared statistic and (b) Deviance G-squared statistic. Is there statistical evidence for lack of fit?

9. Evaluate quasi-Rsquared.
  What is the ratio of the fit to the null model?

10. Evaluate hit ratio.
  What is the classification accuracy of the LRP based on the logistic
  regression?

11. Leave-one-out cross-validaton of hit ratio.
  What is the classification accuracy of the LRP when we use out-of-sample
  predictions?
  
This code to run the steps is wrapped inside a function LRdiagnostics() in a file of the same 
name. It's extracted here for demonstration purposes.

## The arguments of the function are:

SMUlogisticData = Dataframe containing columns: Years, SMU_Esc (annual aggregate escapement to the SMU), and ppn (vector of 0 and 1's indicating if all CUs are above the lower benchmark (1) or not (0); also accepts  proportion of CUs that are above their lower benchmark in each year).

CU_Names = the names of the CUs

All_Ests = Dataframe containing parameters of logistic regression, B_0 and B_1
  with p values (All_Ests <- data.frame(summary(sdreport(obj), p.value=TRUE)))

AggAbund = Vector of scaled observed aggregate abundances, with NAs removed

obsPpnAboveBM = Vectors of 0 and 1's representing if all CUs above their lower benchmarks (1) or not (0) for         Bernoulli regression with NAs removed, or the proportion of CUs above their lower benchmark for binomial           regression

p = the proportion of CUs that must be > their benchmark when defining LRP, for binomial regression only

nLL = negLogLikehood = ans from TMB file, outputted with REPORT(ans); in TMB
  and then called in R with obj$report()$ans

dir = name of directory where plots should be saved followed by /

plotname = filename for residual plots



Step 1.  Box-Tidwell test to assess linearity (Assumption 1). First, data to run the logistic regression are loaded The rda file contain a list, "SMUlogisticData", and vector "CU_Names", and is saved on a "DataIn" directory.

```{r}
load("DataIn/BoxTidwellInput.rda")
#load("BoxTidwellInput.rda")
SMUlogisticData
CU_Names

```

Then generate inputs for logistic regression and run the regression with an additional interaction term, aggregate abundances x ln(aggregate abundance). A significant interaction term indicates that the relationship is not linear.
This code requires that a "Logistic_LRPs_BoxTidwell.cpp" file with a logistic regression that includes this interaction tern has been compiled and is in a directory "TMB_Inputs". A significant Box-Tidwell statistic indicates a lack of linearity in the relationship between aggregate abundances and log-odds.

For references, see pdf at bottom of https://www.researchgate.net/post/Thesis-Help-Binary-Logistic-Regression-Linearity-Assumption
or http://www.statisticalassociates.com/logistic10.htm

<!--I have requested Fox et al. 2015 from the library. Another option for a reference (could request this from library) Fox et al. 2019 https://us.sagepub.com/en-us/nam/an-r-companion-to-applied-regression/book246125#contents-->


```{r boxTidwell, warning=FALSE, message=FALSE, error=FALSE}
  #-------------------------------------------------------------------------------
  # Step 1. Box-Tidwell test to assess linearity between aggregate abundances
  #       and log-odds of all CUs being above thier lower benchmarks. 
  #-------------------------------------------------------------------------------


  Bern_logistic <- FALSE # For this dummy example on WCVI CK, otherwise set to TRUE

  data <- list()
  data$N_Stks <- length(CU_Names)
  digits <- count.dig(SMUlogisticData$SMU_Esc)
  ScaleSMU <- min(10^(digits -1 ), na.rm=T)
  
  data$LM_Agg_Abund <- SMUlogisticData$SMU_Esc/ScaleSMU
  data$LM_Agg_AbundxLn <- SMUlogisticData$SMU_Esc/ScaleSMU * 
    log(SMUlogisticData$SMU_Esc/ScaleSMU)
  data$N_Above_BM <- SMUlogisticData$ppn * data$N_Stks
  data$Pred_Abund <- seq(0, max(data$LM_Agg_Abund)*1.5, 0.1)
  data$p <- 0.95#0.67
  data$Penalty <- as.numeric(FALSE)# Penalty is not relevant with B_2 term.

  data$Bern_logistic <- as.numeric(Bern_logistic)

  data$B_penalty_mu <- mean(c(200,9962))/ScaleSMU
  data$B_penalty_sig <- 2400/ScaleSMU
    
  param <- list()
  param$B_0 <- -2
  param$B_1 <- 0.1
  param$B_2 <- 0.1
  
  dyn.load(dynlib(paste("TMB_Files/Logistic_LRPs_BoxTidwell", sep="")))
    
  obj <- MakeADFun(data, param, DLL="Logistic_LRPs_BoxTidwell", silent=TRUE)
    
  opt <- nlminb(obj$par, obj$fn, obj$gr, 
                control = list(eval.max = 1e5, iter.max = 1e5))
  pl <- obj$env$parList(opt$par) 
  #summary(sdreport(obj), p.value=TRUE)
  All_Ests <- data.frame(summary(sdreport(obj), p.value=TRUE))

  All_Ests$Param <- row.names(All_Ests)
  All_Ests$Param <- sapply(All_Ests$Param, function(x) 
    (unlist(strsplit(x, "[.]"))[[1]]))
    
  BoxTidwellp <- All_Ests %>% filter(Param=="B_2") %>% pull(Pr...z.2..)
  BoxTidwellp <- round(BoxTidwellp,2)    
  names(BoxTidwellp) <- c("Box-Tidwell p-value")
  BoxTidwellp

```

2. Estimate Pearson residuals and deviance residuals, and plot residuals against fitted values. First data are inputted from "Input_LRdiagnostics.rda".


```{r ResidPlot, fig.cap="Plot of Pearson and Deviance residuals against fitted values. Blue line os loess smoothed resudials and grey band is 95 percent confidence intervals.",warning=FALSE, message=FALSE, error=FALSE}

load("DataIn/Input_LRdiagnostics.rda")
All_Ests <- input$All_Ests
AggAbund <- input$AggAbund
obsPpnAboveBM <- input$obsPpnAboveBM
p <- input$p
nLL <- input$nLL
dir <- input$dir
plotname <- input$plotname

source("R/helperFunctions.r") # contains function ggplot.corr()



 #-------------------------------------------------------------------------------
  # Step 2. Estimate Pearson resiudals and deviance residuals. 
  #-------------------------------------------------------------------------------
  
  # Get observed and predicted ppn of CUs above their lower benchmark
  B_0 <- All_Ests %>% filter(Param=="B_0") %>% pull(Estimate)
  B_1 <- All_Ests %>% filter(Param=="B_1") %>% pull(Estimate)
  predPpnAboveBM <- inv_logit(B_0 + B_1*AggAbund)
  
  
  # Pearson residuals: Eq3.15 https://data.princeton.edu/wws509/notes/c3s8
  # setting n=1 (number of trials at each observation of x)
  
  PearResid <- (obsPpnAboveBM - predPpnAboveBM) / sqrt(predPpnAboveBM * 
                                                          (1 - predPpnAboveBM)) 

  # Deviance residual: Eq3.16 https://data.princeton.edu/wws509/notes/c3s8
  # setting n=1 (number of trials at each observation of x)
  # To avoid NANs, use ifelse statement in the eqn, as suggested here:
  # https://www.datascienceblog.net/post/machine-learning/interpreting_
  # generalized_linear_models/#:~:text=For%20type%20%3D%20%22pearson%22%2
  # 0%2C,%CB%86f(xi)
  
  binom.resid <- function(y, mu) {
    y * log( ifelse(y== 0, 1, y/mu)) + (1-y) * log( 
      ifelse(y==1 ,1,(1-y)/(1-mu) ) )  
  }
  
  
  DevResid <- sign(obsPpnAboveBM - predPpnAboveBM ) * 
    sqrt( 2 * binom.resid(y=obsPpnAboveBM, mu=predPpnAboveBM) ) 
  

   
  ## Testing. Residuals match output from R using glm objects
  # ModDat <- data.frame(xx=data$LM_Agg_Abund, yy=SMUlogisticData$ppn)
  # Fit_Mod <- glm( yy ~ xx , family = quasibinomial, data=ModDat)
  # B_0 <- Fit_Mod$coef[1]
  # B_1 <- Fit_Mod$coef[2]
  # obsPpnAboveBM <- ModDat$yy
  # predPpnAboveBM <- inv_logit(B_0 + B_1*ModDat$xx)
  # residuals(Fit_Mod, type="pearson")
  # residuals(Fit_Mod, type="deviance")

  # Put data for diagnostics in a dataframe for plotting
  diagData <- data.frame(predPppnAboveBM = predPpnAboveBM, 
                         PearResid = PearResid, DevResid=DevResid)
  
  p1 <- ggplot(diagData, aes(predPpnAboveBM, PearResid)) +
    geom_point(size=3) + 
    #geom_smooth(method="lm", formula=y~x) + 
    geom_smooth(method="loess", formula=y~x, span=1) + 
    geom_hline(linetype="dashed", yintercept=0) +
    xlab("Predicted proportions") + ylab("Pearson's Residuals") +
    ggtitle("Pearson's Residuals") +
    theme_classic() + 
    theme(axis.text=element_text(size=12),
          axis.title=element_text(size=14,face="bold"),
          plot.title = element_text(size = 20)
    ) 
  
  p2 <- ggplot(diagData, aes(predPpnAboveBM, DevResid)) +
    geom_point(size=3) + 
    #geom_smooth(method="lm", formula=y~x) + 
    geom_smooth(method="loess", formula=y~x, span=1) + 
    geom_hline(linetype="dashed", yintercept=0) +
    xlab("Predicted proportions") + ylab("Deviance Residuals") +
    ggtitle("Deviance Residuals") +
    theme_classic() + 
    theme(axis.text=element_text(size=12),
          axis.title=element_text(size=14,face="bold"),
          plot.title = element_text(size = 20)
    ) 
p1+p2

```

3. Plot autocorrelation among residuals. Are residuals autocorrelated? (Assumption 2)
```{r autocorrPlot, fig.cap="Plot of autocorrelation coefficients for Pearson and Deviance residuals.",warning=FALSE, message=FALSE, error=FALSE}
  #-------------------------------------------------------------------------------
  # Step 3:
  #  Plot autocorrelation among residuals. Are residuals autocorrelated? 
  #   (Assumption 2)
  #-------------------------------------------------------------------------------

 
  # See ggplot.cor function in "helperFunctions.r"
  p3 <- ggplot.corr(data=PearResid, title="Pearsons's residuals") 
  p4 <- ggplot.corr(data=DevResid, title="Deviance residuals") 

  p3+p4

```

4. Are any residuals outliers? (i.e., are deviance residuals >3) (Assumption 3).  Note, we cannot yet evaluate influence test statistics such as Cook's distance because the hat  matrix is not provided in TMB outputs. 


```{r warning=FALSE, message=FALSE, error=FALSE}
  #-------------------------------------------------------------------------------
  # Step 4:
  #  Are any residuals outliers? (i.e., are deviance residuals >3, Assumption 3)
  #   Note, we cannot evaluate influence test statistics such as Cook's 
  #   distance because the hat matrix is not provided in TMB outputs
  #-------------------------------------------------------------------------------
  
  # Observations with a deviance residual in excess of 3 indicate outlier, 
  # Ahmad et al. 2011 (https://www.researchgate.net/publication/260368584_Diagnostic_for_Residual_Outliers_using_Deviance_Component_in_Binary_Logistic_Regression)

  # Possible stricter criteria: Observations with a deviance residual in excess 
  # of two may indicate lack of fit. 
  # https://data.princeton.edu/wws509/notes/c3s8

  # See figure of residuals vs fitted values
  # See figures here, which show that deviance residuals for logistic regression can be >1
  # https://stats.idre.ucla.edu/stata/webbooks/logistic/chapter3/lesson-3-logistic-regression-diagnostics-2/

  # maximum absolute residuals values < 3
  round(max(abs(PearResid)),2)

  round(max(abs(DevResid)), 2)


```
5. Evaluate if sample size is sufficient. As a rule of thumb, Agresti (2007) suggests a minimum of 10 cases with the least frequent outcome for each explanatory variable (1 explanatory variable in this case). For example, if the expected probabilities are 0.50 and 0.50 (for 0 and 1, respectively), then the minimum sample size of at least (10*1) / 0.50 = 20. (https://www.statology.org/assumptions-of-logistic-regression/)

```{r warning=FALSE, message=FALSE, error=FALSE}
  #-------------------------------------------------------------------------------
  # Step 5:
    #  Evaluate if sample size is sufficient
  #-------------------------------------------------------------------------------

  # As a rule of thumb, Agresti (2007) 
  # suggests a minimum of 10 cases with the least frequent outcome for each 
  # explanatory variable (1 explanatory variable in this case, aggregate 
  # abundances). For example, if the frequencies of outcomes are 0.50 and 0.50 
  # (for 0 and 1, respectively), then the minimum sample size of at least 
  # (10*1) / 0.50 = 20.   
  # https://www.statology.org/assumptions-of-logistic-regression/

# Frequency of outcomes (coded to accept proportional as well as 0/1 data)

Freq <- c(sum(floor(SMUlogisticData$ppn))/length(SMUlogisticData$ppn),
          sum(ceiling(SMUlogisticData$ppn))/length(SMUlogisticData$ppn))

minFreq <- min(Freq)
Min.Sample.Size <- 10/min(Freq)


```

3. Are coefficients significantly different than zero (Wald test)?
```{r}
  #-------------------------------------------------------------------------------
  # step 3. 
  # Evaluate statistical significance of model coefficients.
  #-------------------------------------------------------------------------------
  
  signTable <-  All_Ests %>% filter(Param %in% c("B_0", "B_1")) %>% 
    rename(P.value=Pr...z.2..) %>% dplyr::select(Param, Estimate, 
                                                 Std..Error, z.value, 
                                                 P.value)
  
  signTable

```

4. What is goodness of fit based on Pearson chi-square?
```{r}
 #-------------------------------------------------------------------------------
  # Step 4. 
  # Evaluate Pearson Chi-square statistic (goodness of fit). 
  #   Is there statistical evidence for lack of fit?
  #-------------------------------------------------------------------------------
  
  # Evaluate goodness of fit by comparing the residual deviance to a Chi-square
  # distribution  
  
  # Sum of squared Pearson's residuals
  Pearson <- sum(PearResid^2)
  
  # Statistical test of the goodness of fit 
  # Section 6.5.6. https://bookdown.org/roback/bookdown-bysh/ch-logreg.html
  # Section 1.4 of https://www.flutterbys.com.au/stats/tut/tut10.5a.html
  p.PearChiSq <- 1 - pchisq(q=Pearson, df=length(PearResid)-2)
  #values < 0.05 indicate statistically significant evidence for lack of fit
  names(p.PearChiSq) <- c("PearChiSq")
  p.PearChiSq

```

5. What is goodness of fit based on Deviance G-squared? This statistics measures the
deviance of the fitted logistic model with respect to a perfect model (the saturated model)
```{r}
 #-------------------------------------------------------------------------------
  # Step 5. 
  # Evaluate Deviance G-squared statistic (goodness of fit)
  #   Is there statistical evidence for lack of fit?
  #-------------------------------------------------------------------------------
  
  # Deviance statistic (sum of deviance residuals)
  # The deviance is a key concept in logistic regression. It measures the
  # deviance of the fitted logistic model with respect to a perfect model (the 
  # saturated model) https://bookdown.org/egarpor/SSS2-UC3M/logreg-deviance.html
  Deviance <- sum(DevResid^2)
  p.DevChiSq <- 1-pchisq(q=Deviance, df=length(DevResid)-2)
  # values < 0.05 indicate statistically significant evidence for lack of fit
  # See section 6.5.6: https://bookdown.org/roback/bookdown-bysh/ch-logreg.html
  names(p.DevChiSq) <- c("p.DevChiSq")
  
  p.DevChiSq
```

6. What is quasiR2? This is not the percentage of variance explained by the logistic model, but rather a ratio indicating how close is the fit to being perfect  or the worst.
```{r}
  #-------------------------------------------------------------------------------
  # Step 6. 
  # Evaluate quasi-Rsquared.
  #   What is the ratio of the fit to the null model?
  #-------------------------------------------------------------------------------
  
  # What is ratio of fit to the null model? This is a measure of the
  # strength of the relationship
  NullDev <- deviance(glm( obsPpnAboveBM ~ 1 , family = 
                             quasibinomial))
  quasiR2 <- 1 - Deviance/NullDev
  # This is not the percentage of variance explained by the logistic model, 
  # but rather a ratio indicating how close is the fit to being perfect 
  # or the worst. It is not related to any correlation coefficient
  # https://bookdown.org/egarpor/SSS2-UC3M/logreg-deviance.html
  # However, when we forced the y-intercept near zero,  the model actually fits 
  # worse than the null model and quasiR2 can be negative
  names(quasiR2) <- c("quasiR2")
  
  quasiR2

  
  
```

7. Wald test

```{r}
  #-------------------------------------------------------------------------------
  # Step 7. 
  # Evaluate Wald test
  #   Is the predictor, 'Aggregate Abundance', a significant predictor of the ppn
  #   of CUs > their lower benchmark?
  #-------------------------------------------------------------------------------
  
  # The Wald test evaluates the significance of a predictor based on difference 
  # in Deviances between the specificed model and a reduced model without the 
  # predictor
  # See Section 4.7 https://bookdown.org/egarpor/SSS2-UC3M/logreg-deviance.html
  p.Wald <- signif( pchisq(q=(Deviance-NullDev), df=1), digits=2)
  names(p.Wald) <- c("p.Wald")
  p.Wald
  
  
  # REDO THIS...
  # https://www.sciencedirect.com/topics/mathematics/wald-test#:~:text=The%20test%20statistic%20for%20the,follows%20a%20standard%20normal%20distribution.
  
  # https://bookdown.org/egarpor/SSS2-UC3M/logreg-inference.html
  # https://bookdown.org/egarpor/SSS2-UC3M/multlin-inference.html#eq:normp2
  
  # http://web.pdx.edu/~newsomj/mlrclass/ho_significance.pdf
  
  # https://bookdown.org/roback/bookdown-BeyondMLR/ch-logreg.html
  
  # check Wald agaisnt LR test: https://www.ics.uci.edu/~dgillen/STAT211/Handouts/lecture10.pdf
  
  # overall approach: https://online.stat.psu.edu/stat501/lesson/15/15.4
  
```

8. What is the classifiation accuracy?
```{r}
  #-----------------------------------------------------------------------------
  # Step 8. 
  # Evaluate hit ratio from a confusion matrix
  #   What is the classification accuracy of the LRP based on the logistic 
  #   regression?
  #-----------------------------------------------------------------------------
  
  
  # In which years did the model predict aggregate abundances >LRP?
  yHat <- predPpnAboveBM > p
  # In which years were observed aggregate abundances >LRP?
  y <- obsPpnAboveBM > p
  
  # Confusion Matrix
  confMat <- table(y, yHat)
  confMat
  
  # What is the accuracy in classifying observed aggregate abundances?
  # Hit ratio = ratio of correct classification
  hitRatio <- sum(diag(confMat))/sum(confMat)
  hitRatio <- round(hitRatio, digits=2)
  names(hitRatio) <- c("hitRatio")
  hitRatio
  
```

9. Leave-one-out cross validation:  classification accuracy
Sequentially remove one data point at a time, re-estimate logistic regression and LRP, and then calculate Hit Ratio (classification accurarcy) by comparing predicted and observed values for the data point that was held out. See LRdiagnostics.R for details on how to adapt this diagnostic for other case studies.

```{r}
  #------------------------------------------------------------------------------
  # Step 9.
  # Evaluate hit ratio using leave-one-out cross validation from a confusion
  # matrix. Contains 4 sub-steps, 9.1-9.4.
  #   What is the classification accuracy of the LRP using out-of-sample data?
  #------------------------------------------------------------------------------


  # Step 9.1: Estimate logistic regression iteratively, removing a single year
  # each time

  n <- 18 # length of time-series used in logistic regression
  source("R/WCVILRPs.R") # required to use the Get.LRP() function for the 
  # WCVI case study
  
  predPpnAboveBM <- NA

  for (i in 1:n){
    # Estimate logistic regression using function Get.LRP
    zz <- Get.LRP(remove.EnhStocks = TRUE, LOO=i)
    All_Ests <- zz$out$All_Ests

    if(i==1){ # These remain constant over iterations
      # Step 9.2: Get observed time-series of aggregate raw abundances that includes all
      # data and then scale to units near 1-10
      AggAbundRaw <- zz$out$Logistic_Data$xx
      digits <- count.dig(AggAbundRaw)
      ScaleSMU <- min(10^(digits -1 ), na.rm=T)
      AggAbund <- AggAbundRaw/ScaleSMU
      # Get time-series of observed ppns of CUs> benchamark, including all
      # data
      obsPpnAboveBM <- zz$out$Logistic_Data$yy
      # Get threshold p value (ppn of CUs>benchmark) used to estimate LRP
      p <- zz$LRPppn
      #dir <- "DataOut/"
    }

   # Step 9.3: Get predicted ppn of CUs above their lower benchmark for the year
    # that was held out
    B_0 <- All_Ests %>% filter(Param=="B_0") %>% pull(Estimate)
    B_1 <- All_Ests %>% filter(Param=="B_1") %>% pull(Estimate)
    #predPpnAboveBM <- inv_logit(B_0 + B_1*AggAbund)
    predPpnAboveBM[i] <- inv_logit(B_0 + B_1*AggAbund[i])
  } # End of for i in 1:18

  # Step 9.4: Calculate Hit Ratio

  # In which years did the model predict aggregate abundances >LRP?
  yHat <- predPpnAboveBM > p
  # In which years were observed aggregate abundances >LRP?
  y <- obsPpnAboveBM > p

  # Confusion Matrix
  confMat <- table(y, yHat)

  # What is the accuracy in classifying observed aggregate abundances?
  # Hit ratio = ratio of correct classification
  hitRatio <- sum(diag(confMat))/sum(confMat)
  hitRatio <- round(hitRatio, digits=2)

  hitRatio

```

